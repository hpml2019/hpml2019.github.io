---
title: "program"
bg: cyprusmustard
color: white
style: left
fa-icon: calendar
---

<h3 id="papers">Program</h3>

<table id="xxtable">
<tr id="xxtr"><td id="xxtd" colspan="2"><p id="xxhead1">Tuesday, May 14th</p></td></tr>

<tr id="xxtr"><td id="xxtd"><p id="xxp">09:00 09:10</p> </td><td id="xxtd"> <p id="xxbold">Opening session</p><p id="xxp"><a href="open_close_slides.pdf">slides</a></p></td></tr>
<tr id="xxtr"><td id="xxtd"><p id="xxp">09:10 10:00</p> </td><td id="xxtd"> <p id="xxbold"><a href="https://hpml2019.github.io/#keynote">KEYNOTE - Scalable and Distributed DNN Training on Modern HPC Systems: Challenges and Solutions</a></p><p id="xxit">Dhabaleswar K. (DK) Panda</p><p id="xxp">The Ohio State University</p><p id="xxp"><a href="slides/hpml19_keynote_distributed_training_dk.pdf">slides</a></p></td></tr>
<tr id="xxtr"><td id="xxtd"><p id="xxp">10:00 10:30</p> </td><td id="xxtd"> <p id="xxbold">Performance Optimization on Model Synchronization in Parallel Stochastic Gradient Descent Based SVM</p><p id="xxit">Vibhatha Abeykoon, Geoffrey Fox, Minje Kim</p><p id="xxp">Indiana University</p> <p id="xxp"><a id="talk0btn" class="xxa">[More information]</a></p> </td></tr>

<tr id="xxtr"><td id="xxtd"><p id="xxp">10:30 11:00</p> </td><td id="xxtd"><center> <p id="xxp">Coffee Break</p></center></td></tr>

<tr id="xxtr"><td id="xxtd"><p id="xxp">11:00 11:30</p> </td><td id="xxtd"> <p id="xxbold">Distributed MCMC Inference in Dirichlet Process Mixture Models Using Julia</p><p id="xxit">Or Dinari, Angel Yu, Oren Freifeld, John Fisher</p><p id="xxp">Ben-Gurion University of the Negev, Massachusetts Institute of Technology</p> <p id="xxp"><a id="talk1btn" class="xxa">[More information]</a></p> </td></tr>
<tr id="xxtr"><td id="xxtd"><p id="xxp">11:30 12:00</p> </td><td id="xxtd"> <p id="xxbold">TensorFlow on state-of-the-art HPC clusters: a machine learning use case</p><p id="xxit">Guillem Ramirez-Gargallo, Marta Garcia-Gasulla, Filippo Mantovani</p><p id="xxp">Barcelona Supercomputing Center</p> <p id="xxp"><a id="talk2btn" class="xxa">[More information]</a></p> </td></tr>
<tr id="xxtr"><td id="xxtd"><p id="xxp">12:00 12:30</p> </td><td id="xxtd"> <p id="xxbold">Theoretical Scalability Analysis of Distributed Deep Convolutional Neural Networks</p><p id="xxit">Adrián Castelló, Manuel F. Dolz, Enrique S. Quintana-Ortí, Jose Duato</p><p id="xxp">Universitat Politècnica de València, Universitat Jaume I</p> <p id="xxp"><a id="talk3btn" class="xxa">[More information]</a></p> </td></tr>
<tr id="xxtr"><td id="xxtd"><p id="xxp">12:30 13:00</p> </td><td id="xxtd"> <p id="xxbold">An Evaluation Of Transfer Learning for Classifying Sales Engagement Emails at Large Scale</p><p id="xxit">Yong Liu, Pavel Dmitriev, Yifei Huang, Andrew Brooks, Li Dong</p><p id="xxp">Outreach.io</p> <p id="xxp"><a id="talk4btn" class="xxa">[More information]</a></p> </td></tr>

<tr id="xxtr"><td id="xxtd"><p id="xxp">13:00 14:00</p> </td><td id="xxtd"><center> <p id="xxp">Lunch</p></center></td></tr>

<tr id="xxtr"><td id="xxtd"><p id="xxp">14:00 14:50</p> </td><td id="xxtd"> <p id="xxbold">INVITED TALK - Creating Deep Learning Infrastructure for the ARM-Based Flagship Supercomputer</p><p id="xxit">Aleksandr Drozd</p><p id="xxp">Tokyo Institute of Technology</p></td></tr>

<tr id="xxtr"><td id="xxtd"><p id="xxp">14:50 15:20</p> </td><td id="xxtd"> <p id="xxbold">Volumetric Segmentation via Neural Networks Improves Neutron Crystallography Data Analysis</p><p id="xxit">Brendan Sullivan, Rick Archibald, Venu Vandavassi, Patricia Langan, Leighton Coates, Vickie Lynch</p><p id="xxp">Oak Ridge National Laboratory</p> <p id="xxp"><a id="talk5btn" class="xxa">[More information]</a></p> </td></tr>
<tr id="xxtr"><td id="xxtd"><p id="xxp">15:20 15:50</p> </td><td id="xxtd"> <p id="xxbold">A Performance Improvement Approach for Second-Order Optimization in Large Mini-batch Training</p><p id="xxit">Hiroki Naganuma, Rio Yokota</p><p id="xxp">Tokyo Institute of Technology</p> <p id="xxp"><a id="talk6btn" class="xxa">[More information]</a></p> </td></tr>

<tr id="xxtr"><td id="xxtd"><p id="xxp">15:50 16:00</p> </td><td id="xxtd"><p id="xxbold">Closing remarks</p></td></tr>

</table>

<div id="talk0" class="modal">
  <div class="modal-content">
    <span id="close0" class="close">&times;</span>
    <p class="xxblack" id="xxbold">Performance Optimization on Model Synchronization in Parallel Stochastic Gradient Descent Based SVM</p>
    <p class="xxblack" id="xxit">Vibhatha Abeykoon, Geoffrey Fox and Minje Kim</p>
    <p class="xxblack" id="xxp">Indiana University</p>
    &nbsp;
    <p class="xxblack" id="xxbold"><a href="slides/HPML-PSGDSVM.pptx.pdf" style="color:blue">slides</a></p>
    <p class="xxblack" id="xxp">Understanding the bottlenecks in implementing stochastic gradient descent (SGD)-based distributed support vector machines (SVM) algorithm is important in training larger data sets. The communication time to do the model synchronization across the parallel processes is the main bottleneck that causes inefficiency in the training process. The model synchronization is directly affected by the mini-batch size of data processed before the global synchronization. In producing an efficient distributed model, the communication time in training model synchronization has to be as minimum as possible while retaining a high testing accuracy. The effect from model synchronization frequency over the convergence of the algorithm and accuracy of the generated model must be well understood to design an efficient distributed model. In this research, we identify the bottlenecks in model synchronization in parallel stochastic gradient descent (PSGD)-based SVM algorithm with respect to the training model synchronization frequency (MSF). Our research shows that by optimizing the MSF in the data sets that we used, a reduction of 98\% in communication time can be gained (16x - 24x speed up) with respect to high-frequency model synchronization. The training model optimization discussed in this paper guarantees a higher accuracy than the sequential algorithm along with faster convergence.</p>
  </div>
</div>
<div id="talk1" class="modal">
  <div class="modal-content">
    <span id="close1" class="close">&times;</span>
    <p class="xxblack" id="xxbold">Distributed MCMC Inference in Dirichlet Process Mixture Models Using Julia</p>
    <p class="xxblack" id="xxit">Or Dinari, Angel Yu, Oren Freifeld, John Fisher</p>
    <p class="xxblack" id="xxp">Ben-Gurion University of the Negev, MIT</p>
    &nbsp;
    <p class="xxblack" id="xxbold"><a href="slides/talk.pdf" style="color:blue">slides</a></p>
    <p class="xxblack" id="xxp">With the advent of large data sets, the need for general-purpose massively-parallel analysis tools become ever greater. In unsupervised learning, Bayesian nonparametric mixture models, exemplified by the Dirichlet-Process Mixture Model (DPMM), provide a principled Bayesian approach while adapting model complexity to the data. Despite their potential, however, DPMM's have yet to become a popular tool. This is partly due to the lack of friendly software tools that can handle large datasets efficiently. Here we show how, using Julia, one can achieve efficient and easily-modifiable implementation of distributed inference in DPMMs. Particularly, we show how a recent parallel MCMC inference algorithm -- originally implemented in C++ for a single multi-core machine -- can be distributed efficiently across multiple multi-core machines using a distributed-memory model. This leads to speedups, alleviates memory and storage limitations, and lets us learn DPMMs from significantly larger datasets and of higher dimensionality. It also turned out that even on a single machine our Julia implementation handles higher dimensions more gracefully (at least for Gaussians) than the original C++ implementation. Finally, we use our implementation to learn a model of image patches and apply the learned model for image denoising.</p>
  </div>
</div>
<div id="talk2" class="modal">
  <div class="modal-content">
    <span id="close2" class="close">&times;</span>
    <p class="xxblack" id="xxbold">TensorFlow on state-of-the-art HPC clusters: a machine learning use case</p>
    <p class="xxblack" id="xxit">Guillem Ramirez-Gargallo, Marta Garcia-Gasulla, Filippo Mantovani</p>
    <p class="xxblack" id="xxp">Barcelona Supercomputing Center</p>
    &nbsp;
    <p class="xxblack" id="xxbold"><a href="slides/HPML19_TF_on_HPC_clusters_20190514_0828.pdf" style="color:blue">slides</a></p>
    <p class="xxblack" id="xxp">The recent rapid growth of the data-flow programming paradigm enabled the development of specific architectures, e.g., for machine learning. The most known example is the Tensor Processing Unit (TPU) by Google. Standard data-centers, however, still can not foresee large partitions dedicated to machine learning specific architectures. Within data-centers, the High-Performance Computing (HPC) clusters are highly parallel machines targeting a broad class of compute-intensive workflows, as such they can be used for tackling machine learning challenges. On top of this, HPC architectures are rapidly changing, including accelerators and instruction sets other than the classical x86 CPUs. In this blurry scenario, identifying which are the best hardware/software configurations to efficiently support machine learning workloads on HPC clusters is not trivial. In this paper, we considered the workflow of TensorFlow for image recognition. We highlight the strong dependency of the performance in the training phase on the availability of arithmetic libraries optimized for the underlying architecture. Following the example of Intel leveraging the MKL libraries for improving the TensorFlow performance, we plugged the Arm Performance Libraries into TensorFlow and tested on an HPC cluster based on Marvell ThunderX2 CPUs. Also, we performed a scalability study on three state-of-the-art HPC clusters based on different CPU architectures, x86 Intel Skylake, Arm-v8 Marvell ThunderX2, and PowerPC IBM Power9.</p>
  </div>
</div>
<div id="talk3" class="modal">
  <div class="modal-content">
    <span id="close3" class="close">&times;</span>
    <p class="xxblack" id="xxbold">Theoretical Scalability Analysis of Distributed Deep Convolutional Neural Networks</p>
    <p class="xxblack" id="xxit">Adrián Castelló, Manuel F. Dolz, Enrique S. Quintana-Ortí, Jose Duato</p>
    <p class="xxblack" id="xxp">Universitat Politècnica de València, Universitat Jaume I</p>
    &nbsp;
    <p class="xxblack" id="xxbold"><a href="slides/slides.pdf" style="color:blue">slides</a></p>
    <p class="xxblack" id="xxp">We analyze the asynthotic performance of the training process of neural network (NN) on clusters in order to determine their scalability. For this purpose, i) we assume a data parallel implementation of the training algorithm, which replicates the model in the cluster nodes and distributes the
    batches among the cluster nodes; ii) we leverage the roofline model to inspect the performance at the node level, taking into account the floating-point unit (FPU) throughput and memory bandwidth; and iii) we consider distinct collective communication schemes that are optimal depending on the message size and underlying network interconnect topology. We then apply the resulting
    performance model to analyze the scalability of several well-known deep convolutional neural networks in different scenarios, varying the batch size, node floating-point throughput, node memory bandwidth, cluster dimension, and interconnect bandwidth.</p>
  </div>
</div>
<div id="talk4" class="modal">
  <div class="modal-content">
    <span id="close4" class="close">&times;</span>
    <p class="xxblack" id="xxbold">An Evaluation Of Transfer Learning for Classifying Sales Engagement Emails at Large Scale</p>
    <p class="xxblack" id="xxit">Yong Liu, Pavel Dmitriev, Yifei Huang, Andrew Brooks, Li Dong</p>
    <p class="xxblack" id="xxp">Outreach.io</p>
    &nbsp;
    <p class="xxblack" id="xxit"><a href="https://arxiv.org/abs/1905.01971" style="color:blue">Authors' preprint on arXiv</a></p>
    <p class="xxblack" id="xxbold"><a href="slides/HPML2019_Cyprus_Final-5-14-2019.pptx" style="color:blue">slides</a></p>
    <p class="xxblack" id="xxp">This paper conducts an empirical investigation to evaluate transfer learning for classifying sales engagement emails arising from digital sales engagement platforms. Given the complexity of content and context of sales engagement, lack of standardized large corpora and benchmarks, limited labeled examples and heterogenous context of intent, this real-world use case poses both a challenge and an opportunity for adopting a transfer learning approach. We propose an evaluation framework to assess a high performance transfer learning (HPTL) approach in three key areas in addition to commonly used accuracy metrics: 1) effective embeddings and pretrained language model usage, 2) minimum labeled samples requirement and 3) transfer learning implementation strategies. We use in-house sales engagement email samples as the experiment dataset, which includes over 3000 emails labeled as positive, objection, unsubscribe, or not-sure. We discuss our findings on evaluating BERT, ELMo, Flair and GloVe embeddings with both feature-based and fine-tuning approaches and their scalability on a GPU cluster with increasingly larger labeled samples. Our results show that fine-tuning of the BERT model outperforms with as few as 300 labeled samples, but underperforms with fewer than 300 labeled samples, relative to all the feature-based approaches using different embeddings.</p>
  </div>
</div>
<div id="talk5" class="modal">
  <div class="modal-content">
    <span id="close5" class="close">&times;</span>
    <p class="xxblack" id="xxbold">Volumetric Segmentation via Neural Networks Improves Neutron Crystallography Data Analysis</p>
    <p class="xxblack" id="xxit">Brendan Sullivan, Rick Archibald, Venu Vandavassi, Patricia Langan, Leighton Coates, Vickie Lynch</p>
    <p class="xxblack" id="xxp">Oak Ridge National Laboratory</p>
    &nbsp;
    <p class="xxblack" id="xxbold"><a href="slides/sullivan_hpml2019.pptx" style="color:blue">slides</a></p>
    <p class="xxblack" id="xxp">Crystallography is the powerhouse technique for molecular structure determination, with applications in fields ranging from energy storage to drug design. Accurate structure determination, however, relies partly on determining the precise locations and integrated intensities of Bragg peaks in the resulting data. Here, we describe a method for Bragg peak integration that is accomplished using neural networks. The network is based on a U-Net and identifies peaks in three-dimensional reciprocal space through segmentation, allowing prediction of the full 3D peak shape from noisy data that is commonly difficult to process. The procedure for generating appropriate training sets is detailed.  Trained networks achieve Dice coefficients of 0.82 and mean IoUs of 0.69. Carrying out integration over entire datasets, it is demonstrated that integrating neural network-predicted peaks results in improved intensity statistics. Furthermore, using a second dataset, the possibility of transfer learning between datasets is shown. Given the ubiquity and growing complexity of crystallography, we anticipate integration by machine learning to play an increasingly important role across the physical sciences. These early results demonstrate the applicability of deep learning techniques for integrating crystallography data and suggest a possible role in the next generation of crystallography experiments.</p>
  </div>
</div>
<div id="talk6" class="modal">
  <div class="modal-content">
    <span id="close6" class="close">&times;</span>
    <p class="xxblack" id="xxbold">A Performance Improvement Approach for Second-Order Optimization in Large Mini-batch Training</p>
    <p class="xxblack" id="xxit">Hiroki Naganuma, Rio Yokota</p>
    <p class="xxblack" id="xxp">Tokyo Institute of Technology</p>
    &nbsp;
    <p class="xxblack" id="xxbold"><a href="slides/[Final]201905_HPML_CCGRID2019.pdf" style="color:blue">slides</a></p>
    <p class="xxblack" id="xxp">Classical learning theory states that when the number of parameters of the model is too large compared to the data, the model will overfit and the generalization performance deteriorates. However, it has been empirically shown that deep neural networks (DNN) can achieve high generalization capability by training with extremely large amount of data and model parameters, which exceeds the predictions of classical learning theory. One drawback of this is that training of DNN requires enormous calculation time. Therefore, it is necessary to reduce the training time through large scale parallelization. Straightforward data-parallelization of DNN degrades convergence and generalization. In the present work, we investigate the possibility of using second order methods to solve this generalization gap in large-batch training. This is motivated by our observation that each mini-batch becomes more statistically stable, and thus the effect of considering the curvature plays a more important role in large-batch training. We have also found that naively adapting the natural gradient method causes the generalization performance to deteriorate further due to the lack of regularization capability. We propose an improved second order method by smoothing the loss function, which allows second order methods to generalize as well as mini-batch SGD.</p>
  </div>
</div>

<script>
var pnum = 7
var modal = []
var btn = []
var span = []
var b2t = []
var c2t = []
for (i = 0; i < pnum; i++) {
    modal[i] = document.getElementById('talk' + i);
    btn[i] = document.getElementById('talk' + i + "btn");
    span[i] = document.getElementById("close" + i);
    b2t['talk' + i + 'btn'] = i;
    c2t['close' + i] = i;
}

// When the user clicks the button, open the modal 
for (k = 0; k < pnum; k++) {
    btn[k].onclick = function() {
        modal[b2t[this.id]].style.display = "block"; 
    }
}

// When the user clicks on <span> (x), close the modal
for (i = 0; i < pnum; i++) {
    span[i].onclick = function() {
        modal[c2t[this.id]].style.display = "none";
    }
}

// When the user clicks anywhere outside of the modal, close it
window.onclick = function(event) {
    for (i = 0; i < pnum; i++) {
        if (event.target == modal[i]) {
            modal[i].style.display = "none";
        }
    }
}
</script>
